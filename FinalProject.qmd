---
title: "MAT434 Final Project - TMDB Data Analysis"
execute: 
  echo: false
author: "Paul Lamy, Software Engineer and Data Scientist"
format: html
---

```{r echo = FALSE}
# library(tensorflow)
# library(keras)
library(reticulate)

library(dplyr)

library(kableExtra)
library(ggplot2)

use_virtualenv("mat434")
```

## STATEMENT OF PURPOSE
This report takes data from TMDB, a massive online database of films, and examines various characteristics of movies and how these relate to the rating, or vote average, of that movie. The goal of this analysis is to determine what characteristics of a movie can be predcitors for the film's rating. Then, two kinds of regression models are constructed, and these can be used to predict the rating of a movie based on those predictor characteristics.

## EXECUTIVE SUMMARY
In this notebook, a set of films from The Movie DataBase was examined, and what characteristics contributed to `vote average`, or the rating, of a film. The goal is to determine what characteristics of a movie contribute to the film's rating. The dataset was loaded in and some changes to the format were made to make it easier to work with. The amount of films for the top 20 language groups was examined, with English, French, Italian, and Japanese films being the most present in this dataset. The relationship between a film's budget and rating was examined, and there was a large cluster of films with budgets $500,000 and greater. For this cluster, the rating was most frequently between 5.0 and 7.5 out of 10. Then, the relationship between a film's runtime and rating was examined. There were not many films with a runtime greater than 300 minutes. There was a major cluster of films between 80 and 150 minutes in length; for this set of films, ratings had a wide distribution, from 3.0 to 8.0 out of 10. The relationship between a film's release date year and rating was examined. As the year increases, the number of films also increased, and thus the distribution of ratings increased as well. Just like previously, the ratings were distributed between 3.0 to 8.0 out of 10. Median ratings were gathered for the genres of a film, and documentary films had the highest rating of about 7.0 out of 10, with history, animation, and war films being the next highest-rating genres. Median ratings were also examined for the original language of a film. The highest-rated films were Zulu, Latin, Amharic, Catalan, and Punjabi. The languages stated above that were most common in the dataset were not present in the median rating vs. language chart; this is perhaps due to less common languages for a movie to be in having less movies and less ratings, and those higher ratings of said films will skew the median towards a higher rating. Then, Decision Tree and Random Forest models were constructed, and these use the predictors of language, runtime, budget, release date, and genre to predict a film's rating. Using the analysis, the language of a film can contribute to the rating, but intentionally using a higher-rating language like Zulu or Amharic is not a direct impact on the rating. Something intentional that would have an impact is budget and runtime; a budget of 500,000USD or greater and a runtime no more than 200 minutes helps to get a higher rating. Release date also was examined, but as the year increases, so does the distribution of ratings.

## INTRODUCTION
There are many factors that go into making a movie, some intentional such as budget and the runtime of the film, and others such as the spoken language of the film and the revenue the film earns. TheMovieDataBase, hereforth TMDB, is an online database of almost every single movie ever created. For reference, the dataset for this project is  about 45000 rows long!  

```{python echo=FALSE}
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import ast 
import tkinter
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
# filling in missing data (impute = taking the best guess) 
from sklearn.impute import SimpleImputer
# z scores, 
from sklearn.preprocessing import StandardScaler, OneHotEncoder
# Knn = look at points nearby a certain point, and see if the point has that characteristic
from sklearn.neighbors import KNeighborsClassifier
# decision tree -> if this, do this; else if that, do that
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import make_scorer
# RandomForest
from sklearn.ensemble import RandomForestRegressor
# gradient boosting model
from sklearn.ensemble import GradientBoostingClassifier
# Pipeline is a transformation the variables go thru
from sklearn.pipeline import Pipeline
# columns are transformed
from sklearn.compose import ColumnTransformer

# accuracy_score = proportion of predictions that are correct, 
# roc_auc_score = comparisons between true positive rate and false positive rate
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score

from scipy import stats
```

The format of the original data needs some adjustment. The original data has columns that are not helpful for the analysis. This data:'

```{python}
data = pd.read_csv('movies_metadata.csv')
data.info()
```

becomes

```{python}
columns_to_drop = ["adult","belongs_to_collection", "homepage", "id", "imdb_id", "original_title", "overview", "popularity", "poster_path", "production_companies", "production_countries", "spoken_languages", "status", "tagline", "video", "vote_count"]
columns_to_keep = ["budget", "genre_names", "original_language", "release_date", "runtime"] 
data.drop(columns=columns_to_drop, inplace=True)
data.info()
```
Here is the head of the data as it stands right now:
```{r}
py$data %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```
`genres` is in an odd JSON/dictionary-esque format. We will clean this up for better use later.
```{python echo=FALSE}
#data['release_date'] = pd.to_datetime(data['release_date'], errors='coerce').astype('float64')
# data['release_date'] = pd.to_datetime(data['release_date'], errors='coerce')
# 
# # errors='coerce' will replace non-numeric values with NaN
# data['budget'] = pd.to_numeric(data['budget'], errors='coerce')
# 
# data['budget'].fillna(data['budget'].median(), inplace=True)
# data['runtime'].fillna(data['runtime'].median(), inplace=True)
# data['original_language'].fillna(data['original_language'].mode()[0], inplace=True)
#data['release_date'] = pd.to_datetime(data['release_date'], errors='coerce').astype('float64')
data['release_date'] = pd.to_datetime(data['release_date'], errors='coerce')
# data['release_date']
# errors='coerce' will replace non-numeric values with NaN
data['budget'] = pd.to_numeric(data['budget'], errors='coerce')
# make 'budget' a float column
# print(data['budget'])
data['budget'].fillna(data['budget'].median(), inplace=True)
data['runtime'].fillna(data['runtime'].median(), inplace=True)
data['original_language'].fillna(data['original_language'].mode()[0], inplace=True)

```

Furthermore, the dataset uses 2 letter codes for the `original_language` column. These are hard to understand, so let's change them to their actual name, in a new column called `language_name`:

```{python}
language_codes = data['original_language'].unique()
language_codes

language_names = {'en': 'English', 'fr': 'French', 'zh': 'Chinese', 'it': 'Italian', 'fa': 'Persian',
                  'nl': 'Dutch', 'de': 'German', 'cn': 'Chinese', 'ar': 'Arabic', 'es': 'Spanish',
                  'ru': 'Russian', 'sv': 'Swedish', 'ja': 'Japanese', 'ko': 'Korean', 'sr': 'Serbian',
                  'bn': 'Bengali', 'he': 'Hebrew', 'pt': 'Portuguese', 'wo': 'Wolof', 'ro': 'Romanian',
                  'hu': 'Hungarian', 'cy': 'Welsh', 'vi': 'Vietnamese', 'cs': 'Czech', 'da': 'Danish',
                  'no': 'Norwegian', 'nb': 'Norwegian Bokm√•l', 'pl': 'Polish', 'el': 'Greek', 'sh': 'Serbo-Croatian',
                  'xx': 'Unknown', 'mk': 'Macedonian', 'bo': 'Tibetan', 'ca': 'Catalan', 'fi': 'Finnish',
                  'th': 'Thai', 'sk': 'Slovak', 'bs': 'Bosnian', 'hi': 'Hindi', 'tr': 'Turkish',
                  'is': 'Icelandic', 'ps': 'Pashto', 'ab': 'Abkhaz', 'eo': 'Esperanto', 'ka': 'Georgian',
                  'mn': 'Mongolian', 'bm': 'Bambara', 'zu': 'Zulu', 'uk': 'Ukrainian', 'af': 'Afrikaans',
                  'la': 'Latin', 'et': 'Estonian', 'ku': 'Kurdish', 'fy': 'Western Frisian', 'lv': 'Latvian',
                  'ta': 'Tamil', 'sl': 'Slovenian', 'tl': 'Tagalog', 'ur': 'Urdu', 'rw': 'Kinyarwanda',
                  'id': 'Indonesian', 'bg': 'Bulgarian', 'mr': 'Marathi', 'lt': 'Lithuanian', 'kk': 'Kazakh',
                  'ms': 'Malay', 'sq': 'Albanian', '104.0': 'Unknown', 'qu': 'Quechua', 'te': 'Telugu',
                  'am': 'Amharic', 'jv': 'Javanese', 'tg': 'Tajik', 'ml': 'Malayalam', 'hr': 'Croatian',
                  'lo': 'Lao', 'ay': 'Aymara', 'kn': 'Kannada', 'eu': 'Basque', 'ne': 'Nepali',
                  'pa': 'Punjabi', 'ky': 'Kyrgyz', 'gl': 'Galician', '68.0': 'Unknown', 'uz': 'Uzbek',
                  'sm': 'Samoan', 'mt': 'Maltese', '82.0': 'Unknown', 'hy': 'Armenian', 'iu': 'Inuktitut',
                  'lb': 'Luxembourgish', 'si': 'Sinhalese'}

# Filter out numbers from the language codes
language_names_dict = {code: language_names[code] for code in language_codes if isinstance(code, str) and not code.replace('.', '').isdigit()}

# Assuming you have a DataFrame called 'df' with a column 'original_language'
data['language_name'] = data['original_language'].map(language_names_dict)

data['language_name']
data.drop("original_language", axis=1)
```

```{python echo=FALSE}
data["genres"] = data["genres"].str.replace("'", '"')
genres_as_dict = data["genres"].apply(lambda x: json.loads(x) if pd.notna(x) else [])
data['genre_names'] = genres_as_dict.apply(lambda genres: [genre['name'] for genre in genres])

# Explode the 'genre_names' column to create separate rows for each genre
exploded_data = data.explode('genre_names')

# Group by 'genre_names' and calculate the median 'vote_average'
median_by_genre = exploded_data.groupby('genre_names')['vote_average'].median().reset_index()

# Display the result
median_by_genre = median_by_genre.dropna(subset=['vote_average'])

data.drop("genres", axis=1)
```

```{python}
data_size = len(data)
num_vars = data.shape[1]

train, test = train_test_split(data, train_size = 0.75, random_state = 434)
```

## EXPLORATORY DATA ANALYSIS

The dataset contains `r py$data_size` data entries and 
`r py$num_vars` variables. The first few entries are as follows:

```{r}
py$data %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```


The following chart displays the number of films for each main language, and displays only the top 20 languages that have the most films.

```{r echo=FALSE}
py$train %>%
  count(language_name) %>%
  top_n(20) %>%
  ggplot() +
  geom_col(aes(x = reorder(language_name, desc(-n)), y=n, fill = language_name)) +
  labs(
    x = "Original Language",
    y = "Count",
    title = "Number of Films For Main Language"
  ) +
  scale_y_log10() +
  theme_minimal() +
  theme(legend.position="none") +  # Optional: Hide legend if not needed
  coord_flip()  # Optional: Flip the coordinates for horizontal bars
```
English is a widely used language in films, so unsurprisingly, the most films in the dataset are in English.

```{python echo=FALSE}
language_names_arr = train["language_name"].unique() 
median_ratings_orig_lang = []

for language in language_names_arr:
  data_subset = train[train['language_name'] == language]['vote_average']
  # median_vote_avg = np.median()
  if not data_subset.empty and not data_subset.isnull().all():
    median_vote_avg = np.median(data_subset)
    median_ratings_orig_lang.append({'language_name': language, 'median_vote':median_vote_avg})
  
median_ratings_orig_lang_df = pd.DataFrame(median_ratings_orig_lang)
```
This chart compares a movie's budget with its average vote:

```{r echo=FALSE}
py$train %>%
  ggplot() +
  geom_point(aes(x = as.numeric(budget), y=vote_average, color = factor(vote_average))) +
  labs(
    x = "Budget",
    y = "Vote Average",
    title = "Film Budget vs. Rating"
  ) +
  scale_x_log10(
    "budget",
    labels = scales::dollar_format()
  ) +
 scale_color_manual(values = colorRampPalette(c("blue", "red"))(length(unique(py$train$vote_average))))+
 theme(legend.position = "none")
```
There is a massive cluster of ratings from 3-7.5.

We can zoom in specifically on this cluster:
```{r echo=FALSE}
py$train %>%
  ggplot() +
  geom_point(aes(x = as.numeric(budget), y=vote_average, color = factor(vote_average))) +
  labs(
    x = "Budget",
    y = "Vote Average",
    title = "Film Budget vs. Rating"
  ) +
  scale_x_log10(
    "budget",
    labels = scales::dollar_format(),
    breaks = c(100000, 500000, 1000000, 5000000, 10000000) 
  ) +
  xlim(1000000, 10000000) +
 scale_color_manual(values = colorRampPalette(c("blue", "red"))(length(unique(py$train$vote_average))))+
 theme(legend.position = "none")
```

In this zoomed in chart, at intervals/breaks, there is a long vertical line of points with the same budget amount. This could be movies of similar characteristics; for example, maybe all of these films are from a specific production company and thus had the same budget.

This chart compares a movie's runtime with its average vote:

```{r echo=FALSE}
py$train %>%
  ggplot() +
  geom_point(aes(x = as.numeric(runtime), y=vote_average)) +
  labs(
    x = "Runtime",
    y = "Vote Average",
    title = "Runtime of a Film vs. Rating"
  )
```
Let's make a few more charts that zoom in specifically on the major left-skew cluster:

```{r echo=FALSE}
py$train %>%
  ggplot() +
  geom_point(aes(x = as.numeric(runtime), y=vote_average)) +
  labs(
    x = "Runtime",
    y = "Vote Average",
    title = "Runtime of a Film vs. Rating"
  ) +
  scale_x_log10(
    "runtime",
    breaks = c(0, 80, 160, 240, 320, 400) 
  ) +
  xlim(0, 400)  
```
```{r echo=FALSE}
py$train %>%
  ggplot() +
  geom_point(aes(x = as.numeric(runtime), y=vote_average)) +
  labs(
    x = "Runtime",
    y = "Vote Average",
    title = "Runtime of a Film vs. Rating"
  ) +
  scale_x_log10(
    "runtime",
    breaks = c(0, 50, 100, 150, 200) 
  ) +
  xlim(0, 200)  
```

This chart shows the relationship between the date a film was released and the average vote:
```{r echo=FALSE}
py$data %>%
  ggplot() +
  geom_point(aes(x = release_date, y=vote_average)) +
  labs(
    x = "Release Date",
    y = "Vote Average",
    title = "Film Release Date vs. Rating"
  )
```
Approximately by the year 1940, there were already many films, but the number of films and ratings for these films started to increase, as there is a right-skew cluster that gets larger as time goes on. 


This chart displays the median vote for each genre:
```{r echo=FALSE}
py$median_by_genre %>%
  ggplot() +
  geom_col(aes(x = reorder(genre_names, vote_average), y = vote_average, fill = genre_names)) +
  labs(
    x = "Genre",
    y = "Median Vote",
    title = "Median Rating of Films By Genre"
  ) +
  theme(legend.position="none") + # remove legend since fill is used 
  coord_flip()  # Flip the coordinates for horizontal bars
```
Documentary, history, animation, and war films get the most love out of any genre! Sadly, this is not the case with horror, science fiction, or TV movies. 

This chart displays the median vote for each original language:
```{r echo=FALSE}
py$median_ratings_orig_lang_df %>%
  count(language_name, median_vote) %>%
  slice_max(order_by = median_vote, n = 20) %>%
  ggplot() +
  geom_col(aes(x = reorder(language_name, median_vote), y = median_vote, fill = language_name)) +
  labs(
    x = "Original Language",
    y = "Median Vote",
    title = "Median Rating of Films For Main Language"
  ) +
  theme(legend.position="none") +
  coord_flip()
```
Zulu, Latin, and Amharic films are not common, and since there may be less films spoken in thesee languages than others more prevalent in the data set, there may be less ratings, and thus the higher ratings for these films may be a reason for their high placement in the chart (especially since Latin is a dead language!).
Also worth noting is that languages such English, French, or Japanese that had the most movies as that `original_language`/`language_name` are nowhere in this chart. The point made above is probably the case as well.  

## Model Construction
In order to see how accurate the prediction of a film's rating is, the models first need to be constructed. Firstly, the `release_date` column will be changed to a format that is better for the modeling pipeline, which is a Unix timestamp. Then, the train data is split into the x and y, where x is all columns except for the `vote_average` and y is the `vote_average` column the model is trying to predict. There is a separation between numerical columns and categorical columns, which is helpful for the pipeline.  

We will utilize 2 different kinds of mathematical models.

The first is Decision tree. The decision tree model looks at points and asks yes or no questions about the points. The 'depth' of the tree is how many questions are asked.

The second is Random Tree Forest Classifier. It uses multiple Decision tree models that are trained on specific data. (multiple trees = a forest).

The modeling workflow for Random Tree Forest is the same as DecisionTree, with the intermediate steps slightly different than the models that were individual models and not ensembles. Here is a brief summary:

1. Make the numerical pipeline. This says how the numbers will be handled by the model. 
2. Make the categorical pipeline. This has already been done above for DT, and the same categorical pipeline is used since categorical values will be handled the same way.
3. Combine the 2 pipelines into a preprocessor. These combined will be used to tell the model how to handle the data.
4. Create the pipeline, which creates the model.
5. Fit the model (allow it to take practice tests).
6. Run the model and let the model make predicitions for the price range for each home (the model has practiced, this is the actual test).
7. Output the results.

Next step is to create the pipeline, or the series of steps for how we will define and set the settings for both of the models. In this notebook, each of the steps for both is described. 

When doing predictions with models, 3 things happens:
Data preprocessing, which is when we organize data to be ready for analysis;
model building which is when we make a prediction algorithm that learns patterns from the data;
and then we evaluate how good the model is by using test data. A pipeline puts all of these steps together in on process 

We first define which relevant columns are numerical and which ones are categorical.
Then we create the pipeline for both Decision Tree. The pipeline for numerical values is different for Decision Tree and Random Tree Forest.

Both model workflows will use the same categorical pipeline, as the way categories are handled will be the same: using SimpleImputer with the most frequent value to fill in missing values, and OneHotEncoder to turn categorical values into True or False.
The preprocessor is what is used to package the numerical and categorical columns together.
After that comes the main pipelines. This combines the preprocessor, which is how the data is handled, with the specific model. 
Then the model must be fit to the data, or 'take practice tests' and have the model 'take the REAL test'. 
After that, we conclude with finding the Root Mean Squared Error of our results for both models.
```{python}
#training and test sets
data['release_date'] = pd.to_datetime(data['release_date'], errors='coerce')
data['release_date'] = data['release_date'].astype('int64') // 10**9  # Convert to Unix timestamp
train, test = train_test_split(data, train_size = 0.75, random_state = 434)

#Split features and response
x_train = train.drop("vote_average", axis = 1)
y_train = train["vote_average"]
x_test = test.drop("vote_average", axis = 1)
y_test = test["vote_average"]

#Create an instance of a model constructor
dt_reg = DecisionTreeRegressor(random_state = 434)

x_train = x_train.drop(["genres", "title"], axis=1)
x_train.info()

#Create a Data Transformation and Model Pipeline
# columns that have numerical values we are interested in
num_cols = ["budget", "release_date", "revenue", "runtime"]
# columns that have categorical values we are interested in
# cat_cols = ["language_name", "genre_names"]
cat_cols = ["language_name"]


cat_pipe = Pipeline([
    ("cat_imputer", SimpleImputer(strategy = "most_frequent")),
    ("one_hot", OneHotEncoder(handle_unknown='ignore'))
])
num_pipe = Pipeline([
    ("num_imputer", SimpleImputer(strategy = "median")),
    ("norm", StandardScaler())
])

preprocessor = ColumnTransformer([
  ("num", num_pipe, num_cols),
  ("cat", cat_pipe, cat_cols)
])

pipe = Pipeline([
    ("preprocessing", preprocessor),
    ("model", dt_reg)
])

# Define a custom scoring function for neg_root_mean_squared_error
def neg_root_mean_squared_error(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    return -np.sqrt(mse)

# Make it a scorer
neg_root_mean_squared_error_scorer = make_scorer(neg_root_mean_squared_error)
neg_root_mean_squared_error_scorer

y_train = y_train.fillna(y_train.median())
y_test = y_test.fillna(y_test.median())

# This line causes issues
cv_results = cross_val_score(pipe, x_train, y_train, cv=5, scoring=neg_root_mean_squared_error_scorer)

#Collect cross-validation results
cv_results
cv_results.mean()

#Fit model to training data
pipe.fit(x_train, y_train)

#Assess model on test data
test_preds = pipe.predict(x_test)
dt_rmse = np.sqrt(mean_squared_error(y_test, test_preds))
```



The Root Mean Squared Error for DecisionTreeRegressor is `r py$dt_rmse`.

The above format for model construction is followed for RandomTreeForsest as well.

```{python echo=FALSE}
rf_reg = RandomForestRegressor(max_depth=5, random_state=434)

# pipeline for Random Tree Forst
num_pipe_rtf = Pipeline([
  # SimpleImputer = imputes missing values by using the median
  ("num_imputer", SimpleImputer(strategy = "median")),
])

# cat_pipe = pipeline for processing categorical values
cat_pipe = Pipeline([
  # SimpleImputer = imputes categorical values by using the most frequent vals.
  ("cat_imputer", SimpleImputer(strategy = "most_frequent")),
  # oneHotEncoder = converts categorical features into binary (0 = false or 1 = true)
  ("one-hot", OneHotEncoder(handle_unknown = "ignore"))
])

preprocessor_rtf = ColumnTransformer([
  ("num_cols", num_pipe_rtf, num_cols),
  # applies cat_pipe to categorical values
  ("cat_cols", cat_pipe, cat_cols)
])

pipe_rtf = Pipeline([
  ("preprocessor", preprocessor_rtf),
  ("model", rf_reg)
])

cv_results = cross_val_score(pipe_rtf, x_train, y_train, cv=5, scoring=neg_root_mean_squared_error_scorer)

#Collect cross-validation results
cv_results
cv_results.mean()

#Fit model to training data
pipe.fit(x_train, y_train)

#Assess model on test data
test_preds = pipe.predict(x_test)
rtf_rmse = np.sqrt(mean_squared_error(y_test, test_preds))
```

The Root Mean Squared Error for RandomForestRegressor is `r py$rtf_rmse`.

## MODEL INTERPRETATION AND INFERENCE
These models can be used by film people - directors, film crews, or even film buffs - to use the predictors to see what a film's rating will be based on those predictors.

## CONCLUSION
In this notebook, data analysis was done on a set of `r py$data_size` entries from TheMovieDataBase (TMDB) to see what characteristics. EDA was done to exaine trends based on language, runtime, release date, and budget. Then, a model for both DecisionTreeRegressor and RandomForestRegressor were constructed, and these can be used for predicting the rating of a movie using those predictors. The data was cleaned up for better use by the data scientist; however, a column of interest was omitted. The one in question is `production_companies`. This was omitted due to time constraints and a lack of success in trying to extract the name of the production companies, as the format was in an odd JSON/dictionary-esque format, similar to other columns. The director of the movie was also included in the data set, but also in this format which was difficult to work with. This would have also been a major predictor for a film's rating. In the original Kaggle link, only one of the files was used. There was another file that contains the cast, and this could have been examined using tokenization andtext analysis. Due to time constraints and also the fact that this information was stored in a separate file than the one primarily used, this was skipped for now.

## REFERENCES
https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv 
